{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 映画のレビュー検索\n",
    "- 有用なレビューデータの抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import csv\n",
    "import re\n",
    "import json\n",
    "\n",
    "remove=[]\n",
    "result={}\n",
    "\n",
    "with open('/mnt/data/movie_ds/data/rankings.CSV') as f:\n",
    "\n",
    "    #辞書形式\n",
    "    f_matome = csv.reader(f, delimiter=\",\", doublequote=True, lineterminator=\"\\r\\n\", quotechar='\"', skipinitialspace=True)\n",
    "\n",
    "    for i,text in enumerate(f_matome):\n",
    "        V_count=0\n",
    "        mecab = MeCab.Tagger('-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd')\n",
    "        par = mecab.parseToNode(text[2])\n",
    "        if re.compile('\\d{4}').search(text[2]):\n",
    "            remove.append(text[2])\n",
    "        elif re.compile('ホラー|コメディ|ドラマ|サスペンス|アクション|SF|ＳＦ|ミュージカル').search(text[2]):\n",
    "            result[text[0]]=text[2]\n",
    "        elif re.compile('(([mMｍＭ][yYｙＹ])|マイ|まい|ﾏｲ)(・|\\s)*(best|Best|BEST|ｂｅｓｔ|Ｂｅｓｔ|ＢＥＳＴ|worst|Worst|WORST|ＷＯＲＳＴ|Ｗｏｒｓｔ|ｗｏｒｓｔ|([べベ][すス][とト])|ﾍﾞｽﾄ|わーすと|ワースト|ﾜｰｽﾄ)').search(text[2]):\n",
    "            while par:\n",
    "                if par.feature.split(',')[0] == '動詞':\n",
    "                    V_count+=1\n",
    "                par = par.next\n",
    "            if V_count!=0:\n",
    "                result[text[0]]=text[2]\n",
    "            else:\n",
    "                remove.append(text[2])\n",
    "        else:\n",
    "            result[text[0]]=text[2]\n",
    "with open('matome_selected.json', mode='w', encoding='utf-8') as file:\n",
    "    json.dump(result, file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 映画のまとめデータの整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "with open('/mnt/data/movie_ds/data/rankings_cinemas.CSV') as f:\n",
    "    f_movie = csv.reader(f, delimiter=\",\", doublequote=True, lineterminator=\"\\r\\n\", quotechar='\"', skipinitialspace=True)\n",
    "    matome={}\n",
    "    last_id=0\n",
    "    \n",
    "    for i,now in enumerate(f_movie):\n",
    "       \n",
    "        if now[1] in matome.keys():\n",
    "            matome[now[1]].append(now[3])\n",
    "        else:\n",
    "            matome[now[1]]=[now[3]]\n",
    "        \n",
    "with open('matome_dic.json', mode='w', encoding='utf-8') as file:\n",
    "    json.dump(matome, file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- movie idと映画名の対応付け"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/data/movie_ds/data/cinemas.CSV') as f:\n",
    "    movie = csv.reader(f, delimiter=\",\", doublequote=True, lineterminator=\"\\r\\n\", quotechar='\"', skipinitialspace=True)\n",
    "    movie_dic={}\n",
    "    \n",
    "    for i,now in enumerate(movie):\n",
    "        movie_dic[now[0]]=now[1]\n",
    "with open('movie_dic.json', mode='w', encoding='utf-8') as file:\n",
    "    json.dump(movie_dic, file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 各映画の含まれてるまとめを映画ごとにまとめる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "movie_rank={}\n",
    "for k,v in matome.items():\n",
    "    for m in v:\n",
    "        if m in movie_rank.keys():\n",
    "            movie_rank[m].append(k)\n",
    "        else:\n",
    "            movie_rank[m]=[k]\n",
    "sortedDict = sorted(movie_rank.items(), key=lambda x: len(x[1]))\n",
    "sort_dic=[]\n",
    "for k,v in sortedDict:\n",
    "    if k in movie_dic.keys():\n",
    "        matome_title=[]\n",
    "        for matome_id in v:\n",
    "            if matome_id in result.keys():\n",
    "                matome_title.append(result[matome_id].replace('　',' '))\n",
    "        sort_dic.append((movie_dic[k].replace('　',' '),matome_title))\n",
    "with open('sorted_dic.json', mode='w', encoding='utf-8') as file:\n",
    "    json.dump(sort_dic, file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import csv\n",
    "import re\n",
    "import json\n",
    "\n",
    "json_open = open('matome_dic.json', 'r')\n",
    "matome = json.load(json_open)\n",
    "movie_rank={}\n",
    "for k,v in matome.items():\n",
    "    for m in v:\n",
    "        if m in movie_rank.keys():\n",
    "            movie_rank[m].append(k)\n",
    "        else:\n",
    "            movie_rank[m]=[k]\n",
    "sortedDict = sorted(movie_rank.items(), key=lambda x: len(x[1]))\n",
    "with open('movie_to_matome.json', mode='w', encoding='utf-8') as file:\n",
    "    json.dump(sortedDict, file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 映画のレビューデータの整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import csv\n",
    "import json\n",
    "\n",
    "with open('/mnt/data/movie_ds/data/reviews.CSV') as f:\n",
    "    f_review = csv.reader(f, delimiter=\",\", doublequote=True, lineterminator=\"\\r\\n\", quotechar='\"', skipinitialspace=True)\n",
    "    review={}\n",
    "    last_id=0\n",
    "    \n",
    "    for i,now in enumerate(f_review):\n",
    "       \n",
    "        if now[1] in review.keys():\n",
    "            review[now[1]].append(now[7])\n",
    "        else:\n",
    "            review[now[1]]=[now[7]]\n",
    "    for k,v in review.items():\n",
    "        review[k]=[''.join(v)]\n",
    "        \n",
    "with open('review_dic.json', mode='w', encoding='utf-8') as file:\n",
    "    json.dump(review, file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BERTを使用する前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import mojimoji\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, \\\n",
    "  RandomSampler, SequentialSampler, TensorDataset\n",
    "\n",
    "def text2bertinput(\n",
    "        input_text=None, \n",
    "        tokenizer=None,\n",
    "        max_seq_len=None):\n",
    "    #入力テキスト内の半角を全角に変換する\n",
    "    input_text = mojimoji.han_to_zen(input_text)\n",
    "    #特殊トークンの付与＆サブワード分割\n",
    "    tokenize_txt = tokenizer.tokenize('[CLS]' + input_text + '[SEP]')\n",
    "    #各id列の作成\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokenize_txt)\n",
    "    mask_ids = [1] * len(token_ids)\n",
    "    segment_ids = [0] * len(token_ids)\n",
    "    #入力テキストの長さがモデルが指定する最大入力長になるまで[PAD]で水増し\n",
    "    while len(token_ids) < max_seq_len:\n",
    "        #[PAD]トークンの付与\n",
    "        token_ids.append(0)\n",
    "        mask_ids.append(0)\n",
    "        segment_ids.append(0)\n",
    "    #入力テキストが既に最大入力長を超えている場合、途中で切る。\n",
    "    token_ids = token_ids[:max_seq_len]\n",
    "    mask_ids = mask_ids[:max_seq_len]\n",
    "    segment_ids = segment_ids[:max_seq_len]\n",
    "    #各idの長さが適切かどうかの確認\n",
    "    assert len(token_ids) == max_seq_len\n",
    "    assert len(mask_ids) == max_seq_len\n",
    "    assert len(segment_ids) == max_seq_len\n",
    "    #各id列をtorch.tensorへ\n",
    "    tensor_token_id = torch.tensor([token_ids], dtype=torch.long)\n",
    "    tensor_mask_id = torch.tensor([mask_ids], dtype=torch.long)\n",
    "    tensor_segment_id = torch.tensor([segment_ids], dtype=torch.long)\n",
    "    return  {'token_ids': tensor_token_id,\n",
    "             'mask_ids': tensor_mask_id,\n",
    "             'segment_ids': tensor_segment_id}\n",
    "\n",
    "#使用するクラス\n",
    "class GetBertOutputVec(transformers.BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = transformers.BertModel(config)\n",
    "        self.init_weights()\n",
    "        #Average_pooling層の定義\n",
    "        self.adaptive_avg_layer = nn.AdaptiveAvgPool2d((1, 768))\n",
    "         \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        labels=None,\n",
    "        output_vec='cls',\n",
    "        return_dict=None\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        '''\n",
    "        引数で指定された各id列をBERTに入力する処理------------------------------\n",
    "        '''\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        '''\n",
    "        BERTモデルから出力されたベクトルに対する処理----------------------------\n",
    "        '''\n",
    "        sequence_output = None\n",
    "        #3-1-a_CLSトークンに対応したベクトルのみを取得したい場合\n",
    "        if output_vec == 'cls':\n",
    "            #outputs[1]は[CLS]トークンに対応したベクトル\n",
    "            #shapeは[batch_size, 768]\n",
    "            sequence_output = outputs[1]\n",
    "              \n",
    "        #3-1-b_平均ベクトルを取得したい場合\n",
    "        if output_vec == 'avg_pooling':\n",
    "            #outputs[0]は全ての入力トークンに対応したベクトルが格納されている\n",
    "            #shapeは[batch_size, 入力長, 768]\n",
    "            pooled_output = outputs[0]\n",
    "            #入力トークンの内、特殊トークン以外のトークンに対応したベクトルの平均を求める\n",
    "            #sequence_outputのshapeは[batch_size, 768]\n",
    "            sequence_output = self.get_average_pooling(input_ids, pooled_output)  \n",
    "        '''\n",
    "        sequence_output([batch_size, 768])を返す--------------------------------\n",
    "        '''\n",
    "        return sequence_output\n",
    "\n",
    "    #平均ベクトルを計算するための関数\n",
    "    def get_average_pooling(self, input_ids, pooled_output):\n",
    "        average_vectors = []\n",
    "        nums_ctos_token = []\n",
    "        batch_size = 0\n",
    "        #3-1-2_ミニバッチからデータを取り出し、CLSからSEPまでのトークンの数を数える\n",
    "        for input_id in input_ids:\n",
    "            input_id = input_id[torch.where(input_id != 0)]\n",
    "            nums_ctos_token.append(len(input_id))\n",
    "        #3-1-3_ミニバッチからデータを一つずつ取り出し、それぞれの平均ベクトルを算出\n",
    "        for output_matrix, num_ctos_token in zip(pooled_output, nums_ctos_token):\n",
    "            #3-1-4_特殊トークン([CLS], [SEP], [PAD])に対応したベクトルの削除\n",
    "            purpose_matrix = output_matrix[1:num_ctos_token-1]\n",
    "            purpose_matrix = purpose_matrix.view(-1, num_ctos_token-2, 768)\n",
    "            #3-1-5_PADが除かれたマトリクスを平均プーリング層へ入力\n",
    "            average_vector = self.adaptive_avg_layer(purpose_matrix)\n",
    "            average_vectors.append(average_vector)\n",
    "            batch_size += 1\n",
    "        #3-1-6_各ベクトルを持つリストをtensorに変換\n",
    "        average_vectors = torch.cat(average_vectors, dim=1)\n",
    "        average_vectors = average_vectors.view(-1)\n",
    "        #3-1-7_ミニバッチに整える。\n",
    "        average_vectors = average_vectors.view(batch_size, 768)\n",
    "        return average_vectors\n",
    "\n",
    "#Tokenizerの定義\n",
    "from transformers import BertJapaneseTokenizer\n",
    "\n",
    "pretrained_path = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(pretrained_path)\n",
    "\n",
    "#GPUの定義\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "\n",
    "# modelの定義\n",
    "model = GetBertOutputVec.from_pretrained(pretrained_path)\n",
    "\n",
    "#モデルをgpuへ\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- レビューをBERTでベクトル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPUtil\n",
    "import pickle\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from bson.binary import Binary\n",
    "import re\n",
    "\n",
    "#関数を使ってテキストの前処理を行う\n",
    "client=MongoClient('127.0.0.1',27017) #ip, port\n",
    "db=client[\"review_data\"] #database name\n",
    "collection=db[\"review_vector\"] #collection name\n",
    "json_open = open('review_dic.json', 'r')\n",
    "review = json.load(json_open)\n",
    "start_count=1\n",
    "csv_header = ['movie_id','review_id','review_num','review_body','vector']\n",
    "for k,v in review.items():\n",
    "    review_count=1\n",
    "    for now in v:\n",
    "        section_count=1\n",
    "        target_text = re.sub('[「」（）()『』]','',now)\n",
    "        target_text=target_text.replace('\\\\n','').replace(' ','')\n",
    "        target_seq=re.split('[。?？!！☆♡]',target_text)\n",
    "        for now_seq in target_seq:\n",
    "            review_vector={}\n",
    "            if len(now_seq)!=0:\n",
    "                with torch.no_grad():\n",
    "                    with open('log.txt', 'w') as fi:\n",
    "                        #確認\n",
    "                #         print('token_ids', target_ids['token_ids'])\n",
    "                #         print('mask_ids', target_ids['mask_ids'])\n",
    "                #         print('segment_ids', target_ids['segment_ids'])\n",
    "\n",
    "                        target_ids = text2bertinput(input_text=now_seq,tokenizer=tokenizer,max_seq_len=512)\n",
    "\n",
    "                        #各入力idをgpuへ\n",
    "                        token_ids = target_ids['token_ids'].to(device)\n",
    "                        mask_ids = target_ids['mask_ids'].to(device)\n",
    "                        segment_ids = target_ids['segment_ids'].to(device)\n",
    "\n",
    "                        #modelに各idを入力する\n",
    "                        try:\n",
    "                            bert_ouputs_average = model(\n",
    "                                    input_ids = token_ids,\n",
    "                                    attention_mask = mask_ids,\n",
    "                                    token_type_ids = segment_ids,\n",
    "                                    output_vec = 'avg_pooling')\n",
    "                            bert_ouput_vecs_non_grad = bert_ouputs_average.detach()\n",
    "                            review_vector['id']=start_count\n",
    "                            review_vector['movie_id']=k\n",
    "                            review_vector['review_id']=review_count\n",
    "                            review_vector['review_num']=section_count\n",
    "                            review_vector['review_body']=now_seq\n",
    "                            review_vector['vector']=Binary(pickle.dumps(bert_ouput_vecs_non_grad[0], protocol=2))\n",
    "                            collection.insert_one(review_vector)\n",
    "                            del token_ids\n",
    "                            del mask_ids\n",
    "                            del segment_ids\n",
    "                            torch.cuda.empty_cache()\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            print(type(e))\n",
    "                    start_count+=1\n",
    "            section_count+=1\n",
    "        review_count+=1\n",
    "#     if start_count > 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 各レビューのベクトルを映画ごとの平均ベクトルに変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client=MongoClient('127.0.0.1',27017) #ip, port\n",
    "db=client[\"review_data\"] #database name\n",
    "collection=db[\"review_vector\"] #collection name\n",
    "collection2=db[\"movie_vector\"] #collection name\n",
    "json_open = open('movie_dic.json', 'r')\n",
    "movie_data = json.load(json_open)\n",
    "count=1\n",
    "test_count=1\n",
    "for movie_id in movie_data.keys():\n",
    "    movie_vector={}\n",
    "    temp_data=[]\n",
    "    temp_np=np.ndarray([])\n",
    "    query={\"movie_id\":movie_id}\n",
    "    cursor=collection.find(query,{\"_id\":0,\"review_id\":1,\"review_num\":1,\"vector\":1})\n",
    "    for now in cursor:\n",
    "        temp_data.append(pickle.loads(now['vector']).to('cpu').detach().numpy().copy())\n",
    "    if len(temp_data)>=10:\n",
    "        temp_np=temp_data\n",
    "        mean_movie=np.mean(temp_np,axis=0)\n",
    "        movie_vector['id'] = count\n",
    "        movie_vector['movie_id']=movie_id\n",
    "        movie_vector['movie_title']=movie_data[movie_id]\n",
    "        movie_vector['vector']=Binary(pickle.dumps(mean_movie, protocol=2))\n",
    "        collection2.insert_one(movie_vector)\n",
    "        count+=1\n",
    "    test_count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- まとめのベクトル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPUtil\n",
    "import json\n",
    "import re\n",
    "import pymongo\n",
    "import torch\n",
    "import pickle\n",
    "from pymongo import MongoClient\n",
    "from bson.binary import Binary\n",
    "\n",
    "client = MongoClient('127.0.0.1',27017)\n",
    "db=client[\"review_data\"] #database name\n",
    "collection=db[\"matome_vector\"] #collection name\n",
    "#関数を使ってテキストの前処理を行う\n",
    "json_open = open('matome_selected.json', 'r')\n",
    "matome_selected = json.load(json_open)\n",
    "start_count=0\n",
    "for k,v in matome_selected.items():\n",
    "    matome_vectors={}\n",
    "    target_text = re.sub('[。?？!！☆★♡♥「」（）()♪・＆&/／]','',v)\n",
    "    target_text = re.sub('([mMｍＭ][yYｙＹ])|マイ|まい|ﾏｲ|私の','',target_text)\n",
    "    target_text = re.sub('best|Best|BEST|ｂｅｓｔ|Ｂｅｓｔ|ＢＥＳＴ|worst|Worst|WORST|ＷＯＲＳＴ|Ｗｏｒｓｔ|ｗｏｒｓｔ|([べベ][すス][とト])|ﾍﾞｽﾄ|わーすと|ワースト|ﾜｰｽﾄ|TOP|top|Top|ＴＯＰ|ｔｏｐ|Ｔｏｐ|トップ|とっぷ','',target_text)\n",
    "    target_text = re.sub('(10|１０)|(10選|１０選|十選)','',target_text)\n",
    "    target_text = re.sub('(邦画|邦画編)|(洋画|洋画編)','',target_text)\n",
    "    target_text = re.sub('ムービー|むーびー','',target_text)\n",
    "    target_text = re.sub('DVD|ＤＶＤ','',target_text)\n",
    "    target_text = re.sub('化','',target_text)\n",
    "    target_text = re.sub('希望','',target_text)\n",
    "    target_text = re.sub('作品','',target_text)\n",
    "    target_text=target_text.replace('\\u3000','').replace('好きな','').replace('映画','').replace(' ','')\n",
    "    with torch.no_grad():\n",
    "        if len(target_text)!=0:\n",
    "                target_ids = text2bertinput(input_text=target_text,tokenizer=tokenizer,max_seq_len=512)\n",
    "\n",
    "                            #各入力idをgpuへ\n",
    "                token_ids = target_ids['token_ids'].to(device)\n",
    "                mask_ids = target_ids['mask_ids'].to(device)\n",
    "                segment_ids = target_ids['segment_ids'].to(device)\n",
    "\n",
    "                            #modelに各idを入力する\n",
    "                bert_ouputs_average = model(\n",
    "                    input_ids = token_ids,\n",
    "                    attention_mask = mask_ids,\n",
    "                    token_type_ids = segment_ids,\n",
    "                    output_vec = 'avg_pooling')\n",
    "                bert_ouput_vecs_non_grad = bert_ouputs_average\n",
    "                matome_vectors['id'] = start_count+1\n",
    "                matome_vectors['matome_id'] = k\n",
    "                matome_vectors['matome_body'] = target_text\n",
    "                matome_vectors['vector'] = Binary(pickle.dumps(bert_ouput_vecs_non_grad[0], protocol=2))\n",
    "                collection.insert_one(matome_vectors)\n",
    "                del token_ids\n",
    "                del mask_ids\n",
    "                del segment_ids\n",
    "                start_count+=1\n",
    "                torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- テストデータ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read from Mongo\n",
    "import torch\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "import pickle\n",
    "from bson.binary import Binary\n",
    "\n",
    "json_open = open('matome_dic.json', 'r')\n",
    "matome_to_movie = json.load(json_open)\n",
    "client=MongoClient('127.0.0.1',27017) #ip, port\n",
    "db=client[\"review_data\"] #database name\n",
    "combine_collection=db[\"combine_vector\"] #collection name\n",
    "matome_collection=db[\"matome_vector\"] #collection name\n",
    "review_collection=db[\"review_vector\"] #collection name\n",
    "#query={} #if you want to use only some data in database, use query to find them\n",
    "cursor=matome_collection.find({},{\"_id\":0,\"matome_id\":1,\"vector\":1})\n",
    "\n",
    "matome_vectors=[]\n",
    "for cur in cursor:\n",
    "    dic=cur\n",
    "    matome_vectors.append(dic)\n",
    "\n",
    "temp_dic={}\n",
    "used_movie_id=[]\n",
    "count=1\n",
    "for i,now_dic in enumerate(matome_vectors):\n",
    "    review_vectors=[]\n",
    "    if now_dic['matome_id'] in matome_to_movie.keys():\n",
    "        used_movie_id.extend(matome_to_movie[now_dic['matome_id']])\n",
    "    query={\"movie_id\":now_dic['matome_id']} #if you want to use only some data in database, use query to find them\n",
    "    review_cursor=review_collection.find(query,{\"_id\":0,\"review_id\":1,\"review_num\":1,\"vector\":1})\n",
    "    for cur in review_cursor:\n",
    "        dic=cur\n",
    "        review_vectors.append(dic)\n",
    "    for j,temp_dic in enumerate(review_vectors):\n",
    "        combine_vectors={}\n",
    "        c_vec = torch.cat((pickle.loads(now_dic['vector']),pickle.loads(temp_dic['vector'])),0)\n",
    "        combine_vectors['id'] = count\n",
    "        combine_vectors['vector'] = Binary( pickle.dumps(c_vec, protocol=2))\n",
    "        combine_vectors['label'] = 1\n",
    "        combine_collection.insert_one(combine_vectors)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "import pickle\n",
    "from bson.binary import Binary\n",
    "\n",
    "json_open = open('movie_dic.json', 'r')\n",
    "movie_data = json.load(json_open)\n",
    "client=MongoClient('133.2.210.24',27017) #ip, port\n",
    "db=client[\"review_data\"] #database name\n",
    "matome_collection=db[\"matome_vector\"] #collection name\n",
    "combine_collection=db[\"combine_vector\"] #collection name\n",
    "review_collection=db[\"review_vector\"] #collection name\n",
    "#query={} #if you want to use only some data in database, use query to find them\n",
    "cursor=collection.find({},{\"_id\":0,\"matome_id\":1,\"vector\":1})\n",
    "\n",
    "for cur in cursor:\n",
    "    matome_data=cur\n",
    "    i=0\n",
    "    while i<5:\n",
    "        review_vectors=[]\n",
    "        rand_movie_id=random.choice(list(movie_data.keys()))\n",
    "        while rand_movie_id in used_movie_id:\n",
    "            rand_movie_id=random.choice(list(movie_data.keys()))\n",
    "        query={\"movie_id\":rand_movie_id} #if you want to use only some data in database, use query to find them\n",
    "        review_cursor=review_collection.find(query,{\"_id\":0,\"review_id\":1,\"review_num\":1,\"vector\":1})\n",
    "        for cur in review_cursor:\n",
    "            dic=cur\n",
    "            review_vectors.append(dic)\n",
    "        for j,temp_dic in enumerate(review_vectors):\n",
    "            combine_vectors={}\n",
    "            c_vec = torch.cat((pickle.loads(matome_data['vector']),pickle.loads(temp_dic['vector'])),0)\n",
    "            combine_vectors['id'] = count\n",
    "            combine_vectors['vector'] = Binary( pickle.dumps(c_vec, protocol=2))\n",
    "            combine_vectors['label'] = 0        \n",
    "            combine_collection.insert_one(combine_vectors)\n",
    "            count+=1\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- メタデータのベクトル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "import pickle\n",
    "from bson.binary import Binary\n",
    "\n",
    "client=MongoClient('133.2.210.24',27017) #ip, port\n",
    "db=client[\"meta_data\"] #database name\n",
    "collection=db[\"story\"] #collection name\n",
    "vector_collection=db[\"story_vector\"] #collection name\n",
    "#query={} #if you want to use only some data in database, use query to find them\n",
    "cursor=collection.find({},{\"_id\":0,\"タイトル\":1,\"解説\":1,\"あらすじ\":1})\n",
    "\n",
    "story=[]\n",
    "for cur in cursor:\n",
    "    dic=cur\n",
    "    story.append(dic)\n",
    "\n",
    "for now in story:\n",
    "    count=0\n",
    "    story_vector=[]\n",
    "    story_dic={}\n",
    "    mean_commit=0\n",
    "    mean_story=0\n",
    "    if '解説' in now.keys():\n",
    "        section_count=1\n",
    "        target_text = re.sub('[「」（）()『』]','',now['解説'])\n",
    "        target_text=target_text.replace('\\\\n','').replace(' ','')\n",
    "        target_seq=re.split('[。?？!！☆♡]',target_text)\n",
    "        temp_vector=[]\n",
    "        for now_seq in target_seq:\n",
    "            if len(now_seq)!=0:\n",
    "                with torch.no_grad():\n",
    "                    with open('log.txt', 'w') as fi:\n",
    "\n",
    "                        target_ids = text2bertinput(input_text=now_seq,tokenizer=tokenizer,max_seq_len=512)\n",
    "\n",
    "                        #各入力idをgpuへ\n",
    "                        token_ids = target_ids['token_ids'].to(device)\n",
    "                        mask_ids = target_ids['mask_ids'].to(device)\n",
    "                        segment_ids = target_ids['segment_ids'].to(device)\n",
    "\n",
    "                        #modelに各idを入力する\n",
    "                        try:\n",
    "                            bert_ouputs_average = model(\n",
    "                                    input_ids = token_ids,\n",
    "                                    attention_mask = mask_ids,\n",
    "                                    token_type_ids = segment_ids,\n",
    "                                    output_vec = 'avg_pooling')\n",
    "                            bert_ouput_vecs_non_grad = bert_ouputs_average.detach()\n",
    "                            temp_vector.append([bert_ouput_vecs_non_grad])\n",
    "                            del token_ids\n",
    "                            del mask_ids\n",
    "                            del segment_ids\n",
    "                            torch.cuda.empty_cache()\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            print(type(e))\n",
    "        temp_np=np.array(temp_vector) \n",
    "        mean_commit=np.mean(temp_np,axis=0)\n",
    "    if 'あらすじ' in now.keys():\n",
    "        target_text = re.sub('[「」（）()『』]','',now['あらすじ'])\n",
    "        target_text=target_text.replace('\\\\n','').replace(' ','')\n",
    "        target_seq=re.split('[。?？!！☆♡]',target_text)\n",
    "        temp_vector=[]\n",
    "        for now_seq in target_seq:\n",
    "            if len(now_seq)!=0:\n",
    "                with torch.no_grad():\n",
    "                    with open('log.txt', 'w') as fi:\n",
    "\n",
    "                        target_ids = text2bertinput(input_text=now_seq,tokenizer=tokenizer,max_seq_len=512)\n",
    "\n",
    "                        #各入力idをgpuへ\n",
    "                        token_ids = target_ids['token_ids'].to(device)\n",
    "                        mask_ids = target_ids['mask_ids'].to(device)\n",
    "                        segment_ids = target_ids['segment_ids'].to(device)\n",
    "\n",
    "                        #modelに各idを入力する\n",
    "                        try:\n",
    "                            bert_ouputs_average = model(\n",
    "                                    input_ids = token_ids,\n",
    "                                    attention_mask = mask_ids,\n",
    "                                    token_type_ids = segment_ids,\n",
    "                                    output_vec = 'avg_pooling')\n",
    "                            bert_ouput_vecs_non_grad = bert_ouputs_average.detach()\n",
    "                            temp_vector.append([bert_ouput_vecs_non_grad])\n",
    "                            del token_ids\n",
    "                            del mask_ids\n",
    "                            del segment_ids\n",
    "                            torch.cuda.empty_cache()\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            print(type(e))\n",
    "        temp_np=np.array(temp_vector) \n",
    "        mean_story=np.mean(temp_np,axis=0)\n",
    "    if mean_commit != 0:\n",
    "        story_vector.append(mean_commit)\n",
    "    if mean_story != 0:\n",
    "        story_vector.append(mean_story)\n",
    "    story_vector=np.mean(story_vector,axis=0)\n",
    "    story_dic['title'] = now['タイトル']\n",
    "    story_dic['vector'] = Binary( pickle.dumps(story_vector, protocol=2))\n",
    "    vector_collection.insert_one(story_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
